version: 2.1

orbs:
  singularity: singularity/singularity@1.0.11
  codecov: codecov/codecov@1.1.5

commands:

  load-docker-image-archive:
    parameters:
      archive_file:
        type: string
        default: "/tmp/cache/docker/docker.tar.zst"
      run_name:
        type: string
        default: "Load Docker image layer cache"
    steps:
      - run:
          name: "<<parameters.run_name>> from <<parameters.archive_file>>"
          no_output_timeout: 30m
          command: |
            docker info
            set +o pipefail
            if [ -f "<<parameters.archive_file>>" ]; then
              wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
              sudo apt-get update && sudo apt-get -y install zstd
              zstd -d --stdout "<<parameters.archive_file>>" | docker load
              docker images
            fi

  prepare-ds003505-dataset:
    parameters:
      circleci_job:
        type: string
        default: ${CIRCLE_JOB}
    steps:
      - run:
          name: Prepare ds003505 dataset in /tmp/data/<<parameters.circleci_job>>/ds003505
          no_output_timeout: 8h
          command: |
            # Get version, update files.
            #THISVERSION=v$( python /home/circleci/src/connectomemapper3/get_version.py )
            #echo "THISVERSION : ${THISVERSION}"
            # Create the dataset dedicated to this job, make empty code directory
            # and copy the FS license
            mkdir -p "/tmp/data/<<parameters.circleci_job>>/ds003505/code"
            cp -f /tmp/data/ds003505/code/license.txt "/tmp/data/<<parameters.circleci_job>>/ds003505/code/license.txt"
            ls -la /tmp/data/<<parameters.circleci_job>>/ds003505/code
            # Create the test folder to store list of test output files
            mkdir -p /tmp/data/ds003505/test
            # Create dataset subfolders and copy data for sub-01 into them
            echo "Create test-dedicated dataset at: /tmp/data/<<parameters.circleci_job>>/ds003505"
            mkdir -p "/tmp/data/<<parameters.circleci_job>>/ds003505/sub-01/anat"
            mkdir -p "/tmp/data/<<parameters.circleci_job>>/ds003505/derivatives/cmp-v3.1.0/sub-01/eeg"
            mkdir -p "/tmp/data/<<parameters.circleci_job>>/ds003505/derivatives/freesurfer-7.1.1/sub-01"
            mkdir -p "/tmp/data/<<parameters.circleci_job>>/ds003505/derivatives/eeglab-v14.1.1/sub-01/eeg"
            mkdir -p "/tmp/data/<<parameters.circleci_job>>/ds003505/derivatives/cartool-v3.80/sub-01/eeg"
            rsync -pLPv --ignore-errors \
              /tmp/data/ds003505/dataset_description.json \
              "/tmp/data/<<parameters.circleci_job>>/ds003505/dataset_description.json"
            rsync -pLPv --ignore-errors \
              /tmp/data/ds003505/participants.tsv \
              "/tmp/data/<<parameters.circleci_job>>/ds003505/participants.tsv"
            rsync -pLPv --ignore-errors \
              /tmp/data/ds003505/participants.json \
              "/tmp/data/<<parameters.circleci_job>>/ds003505/participants.json"
            rsync -pLPv --ignore-errors \
              /tmp/data/ds003505/derivatives/freesurfer-7.1.1/dataset_description.json \
              "/tmp/data/<<parameters.circleci_job>>/ds003505/derivatives/freesurfer-7.1.1/dataset_description.json"
            rsync -pLPv --ignore-errors \
              /tmp/data/ds003505/derivatives/eeglab-v14.1.1/dataset_description.json \
              "/tmp/data/<<parameters.circleci_job>>/ds003505/derivatives/eeglab-v14.1.1/dataset_description.json"
            rsync -pLPv --ignore-errors \
              /tmp/data/ds003505/derivatives/cartool-v3.80/dataset_description.json \
              "/tmp/data/<<parameters.circleci_job>>/ds003505/derivatives/cartool-v3.80/dataset_description.json"
            rsync -rpLPv --ignore-errors \
              /tmp/data/ds003505/sub-01/anat/* \
              "/tmp/data/<<parameters.circleci_job>>/ds003505/sub-01/anat"
            rsync -rpLPv --ignore-errors \
              /tmp/data/ds003505/derivatives/cmp-v3.1.0/sub-01/eeg/* \
              "/tmp/data/<<parameters.circleci_job>>/ds003505/derivatives/cmp-v3.1.0/sub-01/eeg"
            rsync -rpLPv --ignore-errors \
              /tmp/data/ds003505/derivatives/freesurfer-7.1.1/sub-01/* \
              "/tmp/data/<<parameters.circleci_job>>/ds003505/derivatives/freesurfer-7.1.1/sub-01"
            rm -rvf /tmp/data/<<parameters.circleci_job>>/ds003505/derivatives/freesurfer-7.1.1/sub-01/mri/*.nii.gz
            rsync -rpLPv --ignore-errors \
              /tmp/data/ds003505/derivatives/eeglab-v14.1.1/sub-01/eeg/sub-01_task-faces_*.* \
              "/tmp/data/<<parameters.circleci_job>>/ds003505/derivatives/eeglab-v14.1.1/sub-01/eeg"
            rsync -rpLPv --ignore-errors \
              /tmp/data/ds003505/derivatives/cartool-v3.80/sub-01/eeg/* \
              "/tmp/data/<<parameters.circleci_job>>/ds003505/derivatives/cartool-v3.80/sub-01/eeg"

  prepare-ds-sample-dataset:
    parameters:
      circleci_job:
        type: string
        default: ${CIRCLE_JOB}
      run_name:
        type: string
        default: "Remove any config file present in ds-sample/code and create ds-sample/test"
    steps:
      - run:
          name: <<parameters.run_name>>
          no_output_timeout: 8h
          command: |
            # Get version, update files.
            #THISVERSION=v$( python /home/circleci/src/connectomemapper3/get_version.py )
            #echo "THISVERSION : ${THISVERSION}"
            ls -la  /tmp/data/ds-sample/code
            # Remove existing config files in ds-sample dir
            # to make sure we are using the ones stored in the repo
            rm -f /tmp/data/ds-sample/code/*.ini
            # Create the test folder to store list of test output files
            mkdir -p /tmp/data/ds-sample/test
            # Create the dataset dedicated to this job and copy data into it
            echo "Create test-dedicated dataset at: /tmp/data/<<parameters.circleci_job>>/ds-sample"
            mkdir -p "/tmp/data/<<parameters.circleci_job>>/ds-sample"
            cp -R /tmp/data/ds-sample/* "/tmp/data/<<parameters.circleci_job>>/ds-sample"
            # rsync -rpLPv --ignore-errors /tmp/data/ds-sample/* "/tmp/data/<<parameters.circleci_job>>/ds-sample"

  install-conda-gitannex-and-datalad:
    steps:
      - run:
          name: "Install miniconda in /home/circleci/miniconda and create py37cmp-data environment"
          no_output_timeout: 5h
          command: |
            if [[ ! -d /home/circleci/miniconda ]]; then
              wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh
              bash miniconda.sh -bfp /home/circleci/miniconda
              export PATH="/home/circleci/miniconda/bin:$PATH"
              conda update -y conda
              conda create -n py37cmp-data python=3.7 pip
              source activate py37cmp-data
              rm miniconda.sh
            fi
      - run:
          name: "Install and setup git-annex & DataLad in the environment"
          command: |
            if [[ ! -d /home/circleci/miniconda ]]; then
              export PATH="$HOME/miniconda/bin:$PATH"
              source activate py37cmp-data
              conda install -y -c anaconda -c conda-forge git-annex datalad
              pip install mne
              git config --global user.name 'CMP3 Bot'
              git config --global user.email 'connectomicslab.chuv@gmail.com'
            fi
      - run:
          name: "Install MNE in the environment"
          command: |
            if [[ ! -d /home/circleci/miniconda ]]; then
              export PATH="$HOME/miniconda/bin:$PATH"
              source activate py37cmp-data
              pip install mne
            fi

  install-conda-and-cmp3:
    parameters:
      run_name:
        type: string
        default: "Setup miniconda environment and install connectomemapper3 with Python wrappers"
      cmp3_dir:
        type: string
        default: /home/circleci/src/connectomemapper3
    steps:
      - run:
          name: <<parameters.run_name>>
          no_output_timeout: 5h
          command: |
            wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh
            bash miniconda.sh -bfp $HOME/miniconda
            export PATH="$HOME/miniconda/bin:$PATH"
            conda update -y conda
            conda create -n connectomemapper3 python=3.7 pip=20.1.1 numpy=1.19.2
            source activate connectomemapper3
            echo $(which pip)
            pip install "<<parameters.cmp3_dir>>"
            rm miniconda.sh

  test-singularity-anat-pipeline-wrapper:
    parameters:
      circleci_job:
        type: string
        default: ${CIRCLE_JOB}
      run_name:
        type: string
        default: "Run anatomical pipeline"
      test_name:
        type: string
        default: "test-09"
      config_dir:
        type: string
        default: "/home/circleci/src/connectomemapper3/.circleci/tests/configuration_files"
      singularity_image:
        type: string
        default: "/tmp/cache/singularity/connectomemapper3.simg"
      anat_pipeline_config:
        type: string
        default: "ref_anatomical_config_1.ini"
    steps:
      - run:
          name: <<parameters.run_name>>
          no_output_timeout: 5h
          command: |
            export PATH="$HOME/miniconda/bin:$PATH"
            # Activate the conda environment
            source activate connectomemapper3
            # Execute BIDS App via the singularity python wrapper
            bids_dir="/tmp/data/<<parameters.circleci_job>>/ds-sample"
            output_dir="${bids_dir}/derivatives"
            connectomemapper3_singularity \
                "${bids_dir}" "${output_dir}" participant --participant_label 01 --session_label 01 \
                --singularity_image <<parameters.singularity_image>> \
                --config_dir <<parameters.config_dir>> \
                --anat_pipeline_config <<parameters.anat_pipeline_config>> \
                --fs_license "${bids_dir}/code/license.txt" \
                --ants_number_of_threads 1 \
                --number_of_threads 1 \
                --coverage \
                --notrack \
                --track_carbon_footprint
            # Rename partial coverage
            mv /tmp/data/<<parameters.circleci_job>>/ds-sample/code/coverage.xml /tmp/data/ds-sample/test/<<parameters.test_name>>_coverage.xml
            # Rename execution log
            mv /tmp/data/<<parameters.circleci_job>>/ds-sample/code/log.txt /tmp/data/ds-sample/test/<<parameters.test_name>>_log.txt
            # Rename emissions log
            mv /tmp/data/<<parameters.circleci_job>>/ds-sample/code/emissions.csv /tmp/data/ds-sample/test/<<parameters.test_name>>_co2emissions.csv
      - run:
          name: Checking outputs of Connectome Mapper run (<<parameters.test_name>>)
          command: |
            # Get all files in derivatives except the _*.json interface hash generated by nipype (find) /
            # Remove the full path of the derivatives (sed) / sort the files and write it to a text file
            sudo find /tmp/data/<<parameters.circleci_job>>/ds-sample/derivatives -path */mapflow -prune -o -not -name "_*.json" -type f -print | sed s+/tmp/data/<<parameters.circleci_job>>/ds-sample/derivatives/++ | sort > /tmp/data/ds-sample/test/<<parameters.test_name>>-simg_outputs.out
            diff /home/circleci/src/connectomemapper3/.circleci/tests/expected_outputs/ds-sample_<<parameters.test_name>>-simg_outputs.txt /tmp/data/ds-sample/test/<<parameters.test_name>>-simg_outputs.out
            exit $?

  test-docker-anat-pipeline-wrapper:
    parameters:
      circleci_job:
        type: string
        default: ${CIRCLE_JOB}
      run_name:
        type: string
        default: "Run anatomical pipeline"
      test_name:
        type: string
        default: "test-09"
      config_dir:
        type: string
        default: "/home/circleci/src/connectomemapper3/.circleci/tests/configuration_files"
      anat_pipeline_config:
        type: string
        default: "ref_anatomical_config_1.ini"
      check_output:
        type: boolean
        default: true

    steps:
      - run:
          name: <<parameters.run_name>>
          no_output_timeout: 5h
          command: |
            export PATH="$HOME/miniconda/bin:$PATH"
            # Activate the conda environment
            source activate connectomemapper3
            # Execute BIDS App via the singularity python wrapper
            bids_dir="/tmp/data/<<parameters.circleci_job>>/ds-sample"
            output_dir="${bids_dir}/derivatives"
            connectomemapper3_docker \
                "${bids_dir}" "${output_dir}" participant --participant_label 01 --session_label 01 \
                --docker_image "sebastientourbier/connectomemapper3" \
                --config_dir <<parameters.config_dir>> \
                --anat_pipeline_config <<parameters.anat_pipeline_config>> \
                --fs_license "${bids_dir}/code/license.txt" \
                --ants_number_of_threads 1 \
                --number_of_threads 1 \
                --coverage \
                --notrack \
                --track_carbon_footprint
            # Rename partial coverage
            mv /tmp/data/<<parameters.circleci_job>>/ds-sample/code/coverage.xml /tmp/data/ds-sample/test/<<parameters.test_name>>_coverage.xml
            # Rename execution log
            mv /tmp/data/<<parameters.circleci_job>>/ds-sample/code/log.txt /tmp/data/ds-sample/test/<<parameters.test_name>>_log.txt
            # Rename emissions log
            mv /tmp/data/<<parameters.circleci_job>>/ds-sample/code/emissions.csv /tmp/data/ds-sample/test/<<parameters.test_name>>_co2emissions.csv
      - run:
          name: Checking outputs of Connectome Mapper run (<<parameters.test_name>>)
          command: |
            # Get all files in derivatives except the _*.json interface hash generated by nipype (find) /
            # Remove the full path of the derivatives (sed) / sort the files and write it to a text file
            sudo find /tmp/data/<<parameters.circleci_job>>/ds-sample/derivatives -path */mapflow -prune -o -not -name "_*.json" -type f -print | sed s+/tmp/data/<<parameters.circleci_job>>/ds-sample/derivatives/++ | sort > /tmp/data/ds-sample/test/<<parameters.test_name>>_outputs.out
            diff /home/circleci/src/connectomemapper3/.circleci/tests/expected_outputs/ds-sample_<<parameters.test_name>>_outputs.txt /tmp/data/ds-sample/test/<<parameters.test_name>>_outputs.out
            exit $?

  test-docker-anat-dwi-pipelines-wrapper:
    parameters:
      circleci_job:
        type: string
        default: ${CIRCLE_JOB}
      run_name:
        type: string
        default: "Run anatomical and diffusion pipelines"
      test_name:
        type: string
        default: "test-01"
      config_dir:
        type: string
        default: "/home/circleci/src/connectomemapper3/.circleci/tests/configuration_files"
      anat_pipeline_config:
        type: string
        default: "ref_anatomical_config_1.ini"
      dwi_pipeline_config:
        type: string
        default: "ref_diffusion_config_1.ini"
      check_output:
        type: boolean
        default: true

    steps:
      - run:
          name: <<parameters.run_name>>
          no_output_timeout: 5h
          command: |
            export PATH="$HOME/miniconda/bin:$PATH"
            # Activate the conda environment
            source activate connectomemapper3
            # Execute BIDS App via the singularity python wrapper
            bids_dir="/tmp/data/<<parameters.circleci_job>>/ds-sample"
            output_dir="${bids_dir}/derivatives"
            connectomemapper3_docker \
                "${bids_dir}" "${output_dir}" participant --participant_label 01 --session_label 01 \
                --docker_image "sebastientourbier/connectomemapper3" \
                --config_dir <<parameters.config_dir>> \
                --anat_pipeline_config <<parameters.anat_pipeline_config>> \
                --dwi_pipeline_config <<parameters.dwi_pipeline_config>> \
                --fs_license "${bids_dir}/code/license.txt" \
                --ants_number_of_threads 1 \
                --number_of_threads 1 \
                --coverage \
                --notrack \
                --track_carbon_footprint
            # Rename partial coverage
            mv /tmp/data/<<parameters.circleci_job>>/ds-sample/code/coverage.xml /tmp/data/ds-sample/test/<<parameters.test_name>>_coverage.xml
            # Rename execution log
            mv /tmp/data/<<parameters.circleci_job>>/ds-sample/code/log.txt /tmp/data/ds-sample/test/<<parameters.test_name>>_log.txt
            # Rename emissions log
            mv /tmp/data/<<parameters.circleci_job>>/ds-sample/code/emissions.csv /tmp/data/ds-sample/test/<<parameters.test_name>>_co2emissions.csv
      - run:
          name: Checking outputs of Connectome Mapper run (<<parameters.test_name>>)
          command: |
            # Get all files in derivatives except the _*.json interface hash generated by nipype (find) /
            # Remove the full path of the derivatives (sed) / sort the files and write it to a text file
            sudo find /tmp/data/<<parameters.circleci_job>>/ds-sample/derivatives -path */mapflow -prune -o -not -name "_*.json" -type f -print | sed s+/tmp/data/<<parameters.circleci_job>>/ds-sample/derivatives/++ | sort > /tmp/data/ds-sample/test/<<parameters.test_name>>_outputs.out
            diff /home/circleci/src/connectomemapper3/.circleci/tests/expected_outputs/ds-sample_<<parameters.test_name>>_outputs.txt /tmp/data/ds-sample/test/<<parameters.test_name>>_outputs.out
            exit $?

  test-docker-anat-func-pipelines-wrapper:
    parameters:
      circleci_job:
        type: string
        default: ${CIRCLE_JOB}
      run_name:
        type: string
        default: "Run anatomical and fMRI pipelines"
      test_name:
        type: string
        default: "test-01"
      config_dir:
        type: string
        default: "/home/circleci/src/connectomemapper3/.circleci/tests/configuration_files"
      anat_pipeline_config:
        type: string
        default: "ref_anatomical_config_1.ini"
      func_pipeline_config:
        type: string
        default: "ref_fMRI_config_1.ini"
      check_output:
        type: boolean
        default: true

    steps:
      - run:
          name: <<parameters.run_name>>
          no_output_timeout: 5h
          command: |
            export PATH="$HOME/miniconda/bin:$PATH"
            # Activate the conda environment
            source activate connectomemapper3
            # Execute BIDS App via the singularity python wrapper
            bids_dir="/tmp/data/<<parameters.circleci_job>>/ds-sample"
            output_dir="${bids_dir}/derivatives"
            connectomemapper3_docker \
                "${bids_dir}" "${output_dir}" participant --participant_label 01 --session_label 01 \
                --docker_image "sebastientourbier/connectomemapper3" \
                --config_dir <<parameters.config_dir>> \
                --anat_pipeline_config <<parameters.anat_pipeline_config>> \
                --func_pipeline_config <<parameters.func_pipeline_config>> \
                --fs_license "${bids_dir}/code/license.txt" \
                --ants_number_of_threads 1 \
                --number_of_threads 1 \
                --coverage \
                --notrack \
                --track_carbon_footprint
            # Rename partial coverage
            mv /tmp/data/<<parameters.circleci_job>>/ds-sample/code/coverage.xml /tmp/data/ds-sample/test/<<parameters.test_name>>_coverage.xml
            # Rename execution log
            mv /tmp/data/<<parameters.circleci_job>>/ds-sample/code/log.txt /tmp/data/ds-sample/test/<<parameters.test_name>>_log.txt
            # Rename emissions log
            mv /tmp/data/<<parameters.circleci_job>>/ds-sample/code/emissions.csv /tmp/data/ds-sample/test/<<parameters.test_name>>_co2emissions.csv
      - run:
          name: Checking outputs of Connectome Mapper run (<<parameters.test_name>>)
          command: |
            # Get all files in derivatives except the _*.json interface hash generated by nipype (find) /
            # Remove the full path of the derivatives (sed) / sort the files and write it to a text file
            sudo find /tmp/data/<<parameters.circleci_job>>/ds-sample/derivatives -path */mapflow -prune -o -not -name "_*.json" -type f -print | sed s+/tmp/data/<<parameters.circleci_job>>/ds-sample/derivatives/++ | sort > /tmp/data/ds-sample/test/<<parameters.test_name>>_outputs.out
            diff /home/circleci/src/connectomemapper3/.circleci/tests/expected_outputs/ds-sample_<<parameters.test_name>>_outputs.txt /tmp/data/ds-sample/test/<<parameters.test_name>>_outputs.out
            exit $?

  test-docker-anat-eeg-pipelines-wrapper:
    parameters:
      circleci_job:
        type: string
        default: ${CIRCLE_JOB}
      run_name:
        type: string
        default: "Run anatomical and EEG pipelines"
      test_name:
        type: string
        default: "test-10"
      dataset:
        type: string
        default: "ds003505"
      config_dir:
        type: string
        default: "/home/circleci/src/connectomemapper3/.circleci/tests/configuration_files"
      anat_pipeline_config:
        type: string
        default: "ref_anatomical_config_1.json"
      eeg_pipeline_config:
        type: string
        default: "ref_eeg_config_1.json"
      check_output:
        type: boolean
        default: true

    steps:
      - run:
          name: <<parameters.run_name>>
          no_output_timeout: 5h
          command: |
            export PATH="$HOME/miniconda/bin:$PATH"
            # Activate the conda environment
            source activate connectomemapper3
            # Execute BIDS App via the singularity python wrapper
            bids_dir="/tmp/data/<<parameters.circleci_job>>/<<parameters.dataset>>"
            output_dir="${bids_dir}/derivatives"
            connectomemapper3_docker \
                "${bids_dir}" "${output_dir}" participant --participant_label 01 \
                --docker_image "sebastientourbier/connectomemapper3" \
                --config_dir <<parameters.config_dir>> \
                --anat_pipeline_config <<parameters.anat_pipeline_config>> \
                --eeg_pipeline_config <<parameters.eeg_pipeline_config>> \
                --fs_license "${bids_dir}/code/license.txt" \
                --ants_number_of_threads 1 \
                --number_of_threads 1 \
                --coverage \
                --notrack \
                --track_carbon_footprint
            # Rename partial coverage
            mv /tmp/data/<<parameters.circleci_job>>/<<parameters.dataset>>/code/coverage.xml /tmp/data/<<parameters.dataset>>/test/<<parameters.test_name>>_coverage.xml
            # Rename execution log
            mv /tmp/data/<<parameters.circleci_job>>/<<parameters.dataset>>/code/log.txt /tmp/data/<<parameters.dataset>>/test/<<parameters.test_name>>_log.txt
            # Rename emissions log
            mv /tmp/data/<<parameters.circleci_job>>/<<parameters.dataset>>/code/emissions.csv /tmp/data/<<parameters.dataset>>/test/<<parameters.test_name>>_co2emissions.csv
      - run:
          name: Checking outputs of Connectome Mapper run (<<parameters.test_name>>)
          command: |
            # Get all files in derivatives except the _*.json interface hash generated by nipype (find) /
            # Remove the full path of the derivatives (sed) / sort the files and write it to a text file
            sudo find /tmp/data/<<parameters.circleci_job>>/<<parameters.dataset>>/derivatives -path */mapflow -prune -o -not -name "_*.json" -type f -print | sed s+/tmp/data/<<parameters.circleci_job>>/<<parameters.dataset>>/derivatives/++ | sort > /tmp/data/<<parameters.dataset>>/test/<<parameters.test_name>>_outputs.out
            # diff /home/circleci/src/connectomemapper3/.circleci/tests/expected_outputs/<<parameters.dataset>>_<<parameters.test_name>>_outputs.txt /tmp/data/<<parameters.dataset>>/test/<<parameters.test_name>>_outputs.out
            # exit $?

jobs:

  test-python-install:
    parameters:
      version:
        type: string
        default: latest
    docker:
      - image: circleci/python:<< parameters.version >>
    steps:
      - checkout
      - run:
          name: "Test Install Twine and PyBIDS"
          command: |
            python --version
            pip3 install twine
      - run:
          name: "Smoke Test Install Twine"
          command: |
            python --version
            twine upload -h
      - run:
          name: "Test Wheel Build for setup.py"
          command: |
            python3 setup.py sdist bdist_wheel
      - run:
          name: "Test Install for setup.py"
          command: |
            pip install .
      - run:
          name: "Smoke Test Install for setup.py"
          command: |
            python --version
            pip3 install cmp

  deploy-pypi-release:
    docker:
      - image: circleci/python:3.7
    steps:
      - checkout
      - run:
          name: "Install dependencies"
          command: |
            python --version
            pip3 install twine
      - run:
          name: "Verify git tag vs. version"
          command: |
            python3 setup_pypi.py verify
      - run:
          name: "init .pypirc"
          command: |
            echo -e "[pypi]" >> ~/.pypirc
            echo -e "username = $PYPI_USER" >> ~/.pypirc
            echo -e "password = $PYPI_TOKEN" >> ~/.pypirc
      - run:
          name: "Create distribution wheel"
          command: |
            python3 setup_pypi.py sdist bdist_wheel
      - run:
          name: "Upload to pypi"
          command: |
            twine upload dist/*

  build_docker:
    environment:
      TZ: "/usr/share/zoneinfo/Europe/Zurich"
      SCRATCH: "/scratch"
    machine:
      # NEW: Ubuntu 20.04, docker 20.10.11, docker-compose 1.29.2
      image: ubuntu-2004:202201-02
      # OLD: Ubuntu 16.04, docker 18.09.3, docker-compose 1.23.1
      # image: ubuntu-1604:201903-01
      docker_layer_caching: false
    resource_class: medium
    working_directory: /home/circleci/src/connectomemapper3
    steps:
      - run:
          name: Install parallel zstd
          command: |
            sudo apt-get update
            sudo apt-get install zstd
      - run:
          name: Enable "--squash" feature of docker build
          command: |
            # Enable the --squash feature
            sudo touch /etc/docker/daemon.json
            echo "{\"experimental\": true}" | sudo tee -a /etc/docker/daemon.json
            sudo service docker restart
      #- restore_cache:
      #    keys:
      #      - docker-v1-{{ .Branch }}-{{ .Revision }}
      #      - docker-v1--{{ .Revision }}
      #      - docker-v1-{{ .Branch }}-
      #      - docker-v1-master-
      #      - docker-v1-
      #    paths:
      #      - /tmp/cache
      - checkout:
          path: /home/circleci/src/connectomemapper3
      - run:
          name: Load Docker image layer cache
          no_output_timeout: 30m
          command: |
            docker info
            set +o pipefail
            if [ -f /tmp/cache/docker/docker.tar.zst ]; then
              zstd -d --stdout /tmp/cache/docker/docker.tar.zst | docker load
              docker images
            fi
      - run:
          name: Build Docker image
          no_output_timeout: 120m
          command: |
            # Get version, update files.
            python --version
            THISVERSION=$( python /home/circleci/src/connectomemapper3/get_version.py )
            echo "THISVERSION : ${THISVERSION}"
            echo "CIRCLE_TAG : ${CIRCLE_TAG}"
            if [[ ${THISVERSION:0:1} == "0" ]] ; then
              echo "WARNING: latest git tag could not be found"
              echo "Please, make sure you fetch all tags from upstream with"
              echo "the command ``git fetch --tags --verbose`` and push"
              echo "them to your fork with ``git push origin --tags``"
            fi
            # Build CMP BIDS App docker image
            cd /home/circleci/src/connectomemapper3
            export DOCKER_BUILDKIT=1
            docker build --progress=plain \
              --squash --rm=true \
              --build-arg BUILD_DATE=`date -u +"%Y-%m-%dT%H:%M:%SZ"` \
              --build-arg VCS_REF=`git rev-parse --short HEAD` \
              --build-arg VERSION="${CIRCLE_TAG:-$THISVERSION}" \
              -t sebastientourbier/connectomemapper3 /home/circleci/src/connectomemapper3
      #- run:
      #    name: Prune Docker cache (temporary)
      #    command: docker system prune -f
      #- run:
      #    name: Prune Docker volume (temporary)
      #    command: docker volume prune --force
      - run:
          name: Display storage information
          command: df -h
      # - run:
      #     name: Keep only connectomemapper docker image
      #     command: |
      #       docker rmi $(docker images | grep connectomemapper-ubuntu16.04 | tr -s ' ' | cut -d ' ' -f 3)
      - run:
          name: Save Docker Image
          no_output_timeout: 40m
          command: |
            # Get version, update files.
            THISVERSION=$( python /home/circleci/src/connectomemapper3/get_version.py )
            mkdir -p /tmp/cache/docker
            docker save sebastientourbier/connectomemapper3 \
            | zstd -10 --threads=4 > /tmp/cache/docker/docker.tar.zst
      - persist_to_workspace:
          root: /tmp/cache/docker
          paths:
            - docker.tar.zst
      #- save_cache:
      #    key: docker-v1-{{ .Branch }}-{{ .Revision }}
      #    paths:
      #      - /tmp/cache

  build_singularity:
    machine:
      # Ubuntu 20.04, docker 20.10.11, docker-compose 1.29.2
      image: ubuntu-2004:202201-02
    resource_class: large
    working_directory: /home/circleci/src/connectomemapper3
    steps:
      - checkout:
          path: /home/circleci/src/connectomemapper3
      - run:
          name: "Check whether build_singularity job should be skipped"
          command: |
            cd /home/circleci/src/connectomemapper3
            if [[ "$( git log --format=oneline -n 1 $CIRCLE_SHA1 | grep -i -E '\[skip[ _]?build_singularity]' )" != "" ]]; then
              echo "Skipping singularity build step"
              circleci step halt
            fi
      - attach_workspace:
          at: /tmp/cache/docker
      - run:
          name: "Starting local registry"
          command: docker run -d -p 5000:5000 --restart=always --name registry registry:2
      - load-docker-image-archive:
          archive_file: "/tmp/cache/docker/docker.tar.zst"
      - singularity/install-go:
          go-version: '1.13'
      - singularity/debian-install-3:
          singularity-version: 3.8.4
      - run:
          name: "Set SINGULARITY TMP and CACHEDIR to prevent overflowing the /tmp folder"
          command: |
            mkdir -p ~/.singularity/tmp
            export SINGULARITY_LOCALCACHEDIR=~/.singularity/tmp
            export SINGULARITY_CACHEDIR=~/.singularity/tmp
            export SINGULARITY_TMPDIR=~/.singularity/tmp
            export TMPDIR=~/.singularity/tmp
            export SINGULARITY_DISABLE_CACHE=true
            export
      - run:
          name: "Display storage information ALL"
          command: |
            df -h
      - run:
          name: "Building Singularity image from Docker image"
          command: |
            mkdir -p /tmp/cache/singularity
            docker tag sebastientourbier/connectomemapper3 localhost:5000/sebastientourbier/connectomemapper3:latest
            docker push localhost:5000/sebastientourbier/connectomemapper3:latest
            docker images -a | grep "sebastientourbier/connectomemapper3"
            SINGULARITY_NOHTTPS=1 \
            SINGULARITY_LOCALCACHEDIR=~/.singularity/tmp \
            SINGULARITY_CACHEDIR=~/.singularity/tmp SINGULARITY_TMPDIR=~/.singularity/tmp \
            TMPDIR=~/.singularity/tmp SINGULARITY_DISABLE_CACHE=true \
            singularity build /tmp/cache/singularity/connectomemapper3.simg docker://localhost:5000/sebastientourbier/connectomemapper3:latest
          no_output_timeout: 5h
      - run:
          name: "Display storage information ALL"
          command: df -h
      - persist_to_workspace:
          root: /tmp/cache/singularity
          paths:
            - connectomemapper3.simg

  get_data:
    machine:
      # Ubuntu 20.04, docker 20.10.11, docker-compose 1.29.2
      image: ubuntu-2004:202201-02
    working_directory: /tmp/data
    steps:
      - restore_cache:
          keys:
            - env-v1-{{ .Branch }}-
            - env-v1-master-
            - env-v1-
      - install-conda-gitannex-and-datalad
      - save_cache:
          key: env-v1-{{ .Branch }}-{{ .BuildNum }}
          paths:
            - /home/circleci/miniconda
      - checkout:
          path: /home/circleci/src/connectomemapper3
      - restore_cache:
          keys:
            - data-ds-sample-v2-4-{{ .Branch }}-{{ .Revision }}
            - data-ds-sample-v2-4--{{ .Revision }}
            - data-ds-sample-v2-4-{{ .Branch }}-
            - data-ds-sample-v2-4-master-
            - data-ds-sample-v2-4-
          paths:
            - /tmp/data/ds-sample
      - run:
          name: Get test data from ds-sample
          command: |
            if [[ ! -d /tmp/data/ds-sample ]]; then
              mkdir -p /tmp/data
              wget --retry-connrefused --waitretry=5 --read-timeout=20 --timeout=15 -t 0 -v \
                -O ds-sample.tar.gz "https://zenodo.org/record/5790821/files/ds-sample.tar.gz?download=1"
              tar xvzf ds-sample.tar.gz -C /tmp/data/
              echo "Dataset ds-sample has been successfully downloaded"
            else
              echo "Dataset ds-sample was cached"
            fi
      - run:
          name: Get FreeSurfer derivatives for ds-sample
          command: |
            if [[ ! -d /tmp/data/ds-sample/derivatives/freesurfer-7.1.1 ]]; then
              mkdir -p /tmp/data/ds-sample/derivatives
              wget --retry-connrefused --waitretry=5 --read-timeout=20 --timeout=15 -t 0 -v \
                -O freesurfer.tar.gz "https://zenodo.org/record/5790821/files/freesurfer.tar.gz?download=1"
              tar xvzf freesurfer.tar.gz -C /tmp/data/ds-sample/derivatives
              mv /tmp/data/ds-sample/derivatives/freesurfer /tmp/data/ds-sample/derivatives/freesurfer-7.1.1
              # Remove outdated *.lausanne2008.*.annot files
              rm /tmp/data/ds-sample/derivatives/freesurfer-7.1.1/sub-01_ses-01/label/*.lausanne2008.scale*.annot
              echo "FreeSurfer derivatives of ds-sample have been successfully downloaded"
            else
              echo "FreeSurfer derivatives of ds-sample was cached"
            fi
      - run:
          name: Store FreeSurfer license file
          command: |
            if [[ ! -e /tmp/data/ds-sample/code/license.txt ]]; then
              mkdir -p /tmp/data/ds-sample/code
              cd /tmp/data/ds-sample/code
              echo `echo c2ViYXN0aWVuLnRvdXJiaWVyMUBnbWFpbC5jb20KMzAwNzYKICpDYUpVZ0VMQlJEYkEKIEZTMkkyWDNwNkpicWcK== | base64 -di` > /tmp/data/ds-sample/code/license.txt
              cat /tmp/data/ds-sample/code/license.txt
              echo "FreeSurfer license.txt been successfully created"
            else
              echo "FreeSurfer license.txt was cached"
            fi
      - save_cache:
          key: data-ds-sample-v2-4-{{ .Branch }}-{{ .Revision }}
          paths:
            - /tmp/data/ds-sample
      - restore_cache:
          keys:
            - data-ds003505-v2-3-{{ .Branch }}-{{ .Revision }}
            - data-ds003505-v2-3--{{ .Revision }}
            - data-ds003505-v2-3-{{ .Branch }}-
            - data-ds003505-v2-3-master-
            - data-ds003505-v2-3-
          paths:
            - /tmp/data/ds003505
      - run:
          name: Install ds003505 (VEPCON)
          command: |
            export PATH="$HOME/miniconda/bin:$PATH"
            source activate py37cmp-data
            if [[ ! -d /tmp/data/ds003505 ]]; then
              datalad install https://github.com/OpenNeuroDatasets/ds003505.git
            fi
            # Download content of all files necessary to run anatomical and eeg pipelines on sub-01
            datalad get -J 2 -d ds003505/ ds003505/dataset_description.json
            datalad get -J 2 -d ds003505/ ds003505/participants.json
            datalad get -J 2 -d ds003505/ ds003505/participants.tsv
            datalad get -J 2 -d ds003505/ ds003505/sub-01/anat/*
            datalad get -J 2 -d ds003505/ ds003505/derivatives/*/dataset_description.json
            datalad get -J 2 -d ds003505/ ds003505/derivatives/freesurfer-7.1.1/sub-01/*
            datalad get -J 2 -d ds003505/ ds003505/derivatives/cmp-v3.0.3/sub-01/anat/*
            datalad get -J 2 -d ds003505/ ds003505/derivatives/eeglab-v14.1.1/sub-01/eeg/sub-01_task-faces_*.*
            datalad get -J 2 -d ds003505/ ds003505/derivatives/cartool-v3.80/sub-01/eeg/*
            # Unlock files that might be modified
            datalad unlock -d ds003505/ ds003505/derivatives/cartool-v3.80/dataset_description.json
            datalad unlock -d ds003505/ ds003505/derivatives/eeglab-v14.1.1/dataset_description.json
            datalad unlock -d ds003505/ -r ds003505/derivatives/freesurfer-7.1.1/sub-01/*
            datalad unlock -d ds003505/ ds003505/derivatives/freesurfer-7.1.1/sub-01/tmp/aparc+aseg.mgz
      - run:
          name: Fix dataset_description.json of Cartool and EEGLab derivatives
          command: |
            export PATH="$HOME/miniconda/bin:$PATH"
            source activate py37cmp-data
            cd /home/circleci/src/connectomemapper3
            ls -la /tmp/data/ds003505/derivatives
            ls -la /tmp/data/ds003505/derivatives/cartool-v3.80
            ls -la /tmp/data/ds003505/derivatives/eeglab-v14.1.1
            cat > fix_dataset_description_files.py \<<- "EOF"
            #!/usr/bin/env python3
            import os
            cwd = os.getcwd()
            os.chdir('/home/circleci/src/connectomemapper3')
            from docs.notebooks.EEG_tutorial_utils import fix_vepcon_derivatives_dataset_description_files
            fix_vepcon_derivatives_dataset_description_files('/tmp/data/ds003505')
            os.chdir(cwd)
            EOF
            python fix_dataset_description_files.py
            cat /tmp/data/ds003505/derivatives/cartool-v3.80/dataset_description.json
            cat /tmp/data/ds003505/derivatives/eeglab-v14.1.1/dataset_description.json
      - run:
          name: Create the -trans.fif-file necessary to adjust electrode positions
          command: |
            export PATH="$HOME/miniconda/bin:$PATH"
            source activate py37cmp-data
            cd /home/circleci/src/connectomemapper3
            cat > create_transform_file.py \<<- "EOF"
            #!/usr/bin/env python3
            import os
            cwd = os.getcwd()
            os.chdir('/home/circleci/src/connectomemapper3')
            from docs.notebooks.EEG_tutorial_utils import create_trans_files
            create_trans_files('/tmp/data/ds003505', 'cmp-v3.1.0', 'sub-01')
            os.chdir(cwd)
            EOF
            python create_transform_file.py
      - run:
          name: Store FreeSurfer license file
          command: |
            if [[ ! -e /tmp/data/ds003505/code/license.txt ]]; then
              mkdir -p /tmp/data/ds003505/code
              cd /tmp/data/ds003505/code
              echo `echo c2ViYXN0aWVuLnRvdXJiaWVyMUBnbWFpbC5jb20KMzAwNzYKICpDYUpVZ0VMQlJEYkEKIEZTMkkyWDNwNkpicWcK== | base64 -di` > /tmp/data/ds003505/code/license.txt
              cat /tmp/data/ds003505/code/license.txt
              echo "FreeSurfer license.txt been successfully created"
            else
              echo "FreeSurfer license.txt was cached"
            fi
      - save_cache:
          key: data-ds003505-v2-3-{{ .Branch }}-{{ .Revision }}
          paths:
            - /tmp/data/ds003505
      # - persist_to_workspace:
      #     root: /tmp
      #     paths:
      #       - data/ds003505

  test1_docker_parcellation:
    machine:
      # Ubuntu 20.04, docker 20.10.11, docker-compose 1.29.2
      image: ubuntu-2004:202201-02
    resource_class: large
    working_directory: /tmp/data
    environment:
      - FS_LICENSE: /tmp/data/${CIRCLE_JOB}/ds-sample/code/license.txt
    steps:
      - run:
          name: Get precise CPU specifications
          command: |
            cat  /proc/cpuinfo
      - checkout:
          path: /home/circleci/src/connectomemapper3
      - run:
          name: Check whether test_docker_parcellation should be skipped
          command: |
            cd /home/circleci/src/connectomemapper3
            if [[ "$( git log --format=oneline -n 1 $CIRCLE_SHA1 | grep -i -E '\[skip[ _]?test_docker_parcellation\]' )" != "" ]]; then
              echo "Skipping test_docker_parcellation job"
              circleci step halt
            fi
      - attach_workspace:
          at: /tmp/cache/docker
      - restore_cache:
          keys:
            - data-ds-sample-v2-4-{{ .Branch }}-{{ .Revision }}
            - data-ds-sample-v2-4--{{ .Revision }}
            - data-ds-sample-v2-4-{{ .Branch }}-
            - data-ds-sample-v2-4-master-
            - data-ds-sample-v2-4-
          paths:
            - /tmp/data/ds-sample
      - load-docker-image-archive:
          archive_file: "/tmp/cache/docker/docker.tar.zst"
      - install-conda-and-cmp3:
          run_name: "Setup miniconda environment and install connectomemapper3 with Python wrappers"
          cmp3_dir: /home/circleci/src/connectomemapper3
      - prepare-ds-sample-dataset:
          circleci_job: ${CIRCLE_JOB}
      - test-docker-anat-pipeline-wrapper:
          circleci_job: ${CIRCLE_JOB}
          run_name: Run anatomical pipeline on ds-sample (test 1 - Lausanne 2018 - Python wrapper)
          test_name: "test-01"
          config_dir: /home/circleci/src/connectomemapper3/.circleci/tests/configuration_files
          anat_pipeline_config: "ref_anatomical_config_1.ini"
          check_output : true
      - run:
          name: Clean working directory
          when: always
          command: |
            sudo chown $(id -un):$(id -gn) -R /tmp/data/${CIRCLE_JOB}/ds-sample
            find /tmp/data/ds-sample/derivatives -not -name "*.svg" -not -name "*.html" -not -name "*.rst" \
                -not -name "*.mat" -not -name "*.gpickle" -not -name "*.lta" -not -name "*.json" -not -name "*.txt" \
                -not -name "*.log" -not -name "*.pklz" -type f -delete
      - persist_to_workspace:
          root: /tmp
          paths:
            - data/ds-sample/test/test-01_*
            - data/test1_docker_parcellation/ds-sample/derivatives/nipype-*
            - data/test1_docker_parcellation/ds-sample/derivatives/freesurfer-*
            - data/test1_docker_parcellation/ds-sample/derivatives/cmp-*
      - store_artifacts:
          path: /tmp/data/ds-sample/test
      - store_artifacts:
          path: /tmp/data/test1_docker_parcellation/ds-sample/code
      - store_artifacts:
          path: /tmp/data/test1_docker_parcellation/ds-sample/derivatives/cmp-*
      - store_artifacts:
          path: /tmp/data/test1_docker_parcellation/ds-sample/derivatives/nipype-*

  test2_docker_parcellation:
    machine:
      # Ubuntu 20.04, docker 20.10.11, docker-compose 1.29.2
      image: ubuntu-2004:202201-02
    resource_class: medium
    working_directory: /tmp/data
    environment:
      - FS_LICENSE: /tmp/data/${CIRCLE_JOB}/ds-sample/code/license.txt
    steps:
      - run:
          name: Get precise CPU specifications
          command: |
            cat  /proc/cpuinfo
      - checkout:
          path: /home/circleci/src/connectomemapper3
      - run:
          name: Check whether test_docker_parcellation should be skipped
          command: |
            cd /home/circleci/src/connectomemapper3
            if [[ "$( git log --format=oneline -n 1 $CIRCLE_SHA1 | grep -i -E '\[skip[ _]?test_docker_parcellation\]' )" != "" ]]; then
              echo "Skipping test_docker_parcellation job"
              circleci step halt
            fi
      - attach_workspace:
          at: /tmp/cache/docker
      - restore_cache:
          keys:
            - data-ds-sample-v2-4-{{ .Branch }}-{{ .Revision }}
            - data-ds-sample-v2-4--{{ .Revision }}
            - data-ds-sample-v2-4-{{ .Branch }}-
            - data-ds-sample-v2-4-master-
            - data-ds-sample-v2-4-
          paths:
            - /tmp/data/ds-sample
      - load-docker-image-archive:
          archive_file: "/tmp/cache/docker/docker.tar.zst"
      - install-conda-and-cmp3:
          run_name: "Setup miniconda environment and install connectomemapper3 with Python wrappers"
          cmp3_dir: /home/circleci/src/connectomemapper3
      - prepare-ds-sample-dataset:
          circleci_job: ${CIRCLE_JOB}
      - test-docker-anat-pipeline-wrapper:
          circleci_job: ${CIRCLE_JOB}
          run_name: Run anatomical pipeline on ds-sample (test 2 - NativeFreesurfer)
          test_name: "test-02"
          config_dir: /home/circleci/src/connectomemapper3/.circleci/tests/configuration_files
          anat_pipeline_config: "ref_anatomical_config_2.ini"
          check_output: true
      - run:
          name: Clean working directory
          when: always
          command: |
            sudo chown $(id -un):$(id -gn) -R /tmp/data/${CIRCLE_JOB}/ds-sample
            find /tmp/data/ds-sample/derivatives -not -name "*.svg" -not -name "*.html" -not -name "*.rst" \
                -not -name "*.mat" -not -name "*.gpickle" -not -name "*.lta" -not -name "*.json" -not -name "*.txt" \
                -not -name "*.log" -not -name "*.pklz" -type f -delete
      - persist_to_workspace:
          root: /tmp/data
          paths:
            - ds-sample/test/test-02_*
      - store_artifacts:
          path: /tmp/data/ds-sample/test
      - store_artifacts:
          path: /tmp/data/test2_docker_parcellation/ds-sample/code
      - store_artifacts:
          path: /tmp/data/test2_docker_parcellation/ds-sample/derivatives/cmp-*
      - store_artifacts:
          path: /tmp/data/test2_docker_parcellation/ds-sample/derivatives/nipype-*

  test3_docker_dsi_mrtrix:
    machine:
      # Ubuntu 20.04, docker 20.10.11, docker-compose 1.29.2
      image: ubuntu-2004:202201-02
    resource_class: large
    working_directory: /tmp/data
    environment:
      - FS_LICENSE: /tmp/data/${CIRCLE_JOB}/ds-sample/code/license.txt
    steps:
      - run:
          name: Get precise CPU specifications
          command: |
            cat  /proc/cpuinfo
      - checkout:
          path: /home/circleci/src/connectomemapper3
      - run:
          name: Check whether test_docker_dsi_mrtrix should be skipped
          command: |
            cd /home/circleci/src/connectomemapper3
            if [[ "$( git log --format=oneline -n 1 $CIRCLE_SHA1 | grep -i -E '\[skip[ _]?test_docker_dsi_mrtrix\]' )" != "" ]]; then
              echo "Skipping test_docker_dsi_mrtrix job"
              circleci step halt
            fi
      - attach_workspace:
          at: /tmp/cache/docker
      - restore_cache:
          keys:
            - data-ds-sample-v2-4-{{ .Branch }}-{{ .Revision }}
            - data-ds-sample-v2-4--{{ .Revision }}
            - data-ds-sample-v2-4-{{ .Branch }}-
            - data-ds-sample-v2-4-master-
            - data-ds-sample-v2-4-
          paths:
            - /tmp/data/ds-sample
      - run:
          name: "Copy anatomical pipeline output from test 1"
          command: |
            mkdir -p /tmp/data/${CIRCLE_JOB}/ds-sample/derivatives/cmp-custom
            cp -r /tmp/data/test1_docker_parcellation/ds-sample/derivatives/cmp-*/* \
            /tmp/data/${CIRCLE_JOB}/ds-sample/derivatives/cmp-custom/
      - load-docker-image-archive:
          archive_file: "/tmp/cache/docker/docker.tar.zst"
      - install-conda-and-cmp3:
          run_name: "Setup miniconda environment and install connectomemapper3 with Python wrappers"
          cmp3_dir: /home/circleci/src/connectomemapper3
      - prepare-ds-sample-dataset:
          circleci_job: ${CIRCLE_JOB}
      - test-docker-anat-dwi-pipelines-wrapper:
          circleci_job: ${CIRCLE_JOB}
          run_name: Run anatomical and diffusion pipelines on ds-sample (test 3 - Lausanne2018 + FSL FLIRT + Dipy SHORE + MRtrix SD_STREAM tracking + MRtrix SIFT tractogram filtering)
          test_name: "test-03"
          config_dir: /home/circleci/src/connectomemapper3/.circleci/tests/configuration_files
          anat_pipeline_config: "ref_anatomical_config_3.json"
          dwi_pipeline_config: "ref_diffusion_config_1.ini"
          check_output: true
      - persist_to_workspace:
          root: /tmp/data
          paths:
            - ds-sample/test/test-03_*
      - store_artifacts:
          path: /tmp/data/ds-sample/test
      - store_artifacts:
          path: /tmp/data/test3_docker_dsi_mrtrix/ds-sample/code
      - store_artifacts:
          path: /tmp/data/test3_docker_dsi_mrtrix/ds-sample/derivatives/cmp-*
      - store_artifacts:
          path: /tmp/data/test3_docker_dsi_mrtrix/ds-sample/derivatives/nipype-*

  test4_docker_dsi_mrtrix:
    machine:
      # Ubuntu 20.04, docker 20.10.11, docker-compose 1.29.2
      image: ubuntu-2004:202201-02
    resource_class: large
    working_directory: /tmp/data
    environment:
      - FS_LICENSE: /tmp/data/${CIRCLE_JOB}/ds-sample/code/license.txt
    steps:
      - run:
          name: Get precise CPU specifications
          command: |
            cat  /proc/cpuinfo
      - checkout:
          path: /home/circleci/src/connectomemapper3
      - run:
          name: Check whether test_docker_dsi_mrtrix should be skipped
          command: |
            cd /home/circleci/src/connectomemapper3
            if [[ "$( git log --format=oneline -n 1 $CIRCLE_SHA1 | grep -i -E '\[skip[ _]?test_docker_dsi_mrtrix\]' )" != "" ]]; then
              echo "Skipping test_docker_dsi_mrtrix job"
              circleci step halt
            fi
      - attach_workspace:
          at: /tmp/cache/docker
      - restore_cache:
          keys:
            - data-ds-sample-v2-4-{{ .Branch }}-{{ .Revision }}
            - data-ds-sample-v2-4--{{ .Revision }}
            - data-ds-sample-v2-4-{{ .Branch }}-
            - data-ds-sample-v2-4-master-
            - data-ds-sample-v2-4-
          paths:
            - /tmp/data/ds-sample
      - run:
          name: "Copy anatomical pipeline output from test 1"
          command: |
            mkdir -p /tmp/data/${CIRCLE_JOB}/ds-sample/derivatives
            cp -r /tmp/data/test1_docker_parcellation/ds-sample/derivatives/nipype-* \
            /tmp/data/${CIRCLE_JOB}/ds-sample/derivatives
            rm -r /tmp/data/ds-sample/derivatives/freesurfer-*
            cp -r /tmp/data/test1_docker_parcellation/ds-sample/derivatives/freesurfer-* \
            /tmp/data/${CIRCLE_JOB}/ds-sample/derivatives
      - load-docker-image-archive:
          archive_file: "/tmp/cache/docker/docker.tar.zst"
      - install-conda-and-cmp3:
          run_name: "Setup miniconda environment and install connectomemapper3 with Python wrappers"
          cmp3_dir: /home/circleci/src/connectomemapper3
      - prepare-ds-sample-dataset:
          circleci_job: ${CIRCLE_JOB}
      - test-docker-anat-dwi-pipelines-wrapper:
          circleci_job: ${CIRCLE_JOB}
          run_name: Run anatomical and diffusion pipelines on ds-sample (test 4 - Lausanne2018 + ANTs SyN + Dipy SHORE + MRtrix ACT iFOV2 tracking)
          test_name: "test-04"
          config_dir: /home/circleci/src/connectomemapper3/.circleci/tests/configuration_files
          anat_pipeline_config: "ref_anatomical_config_1.ini"
          dwi_pipeline_config: "ref_diffusion_config_2.ini"
          check_output: true
      - persist_to_workspace:
          root: /tmp/data
          paths:
            - ds-sample/test/test-04_*
      - store_artifacts:
          path: /tmp/data/ds-sample/test
      - store_artifacts:
          path: /tmp/data/test4_docker_dsi_mrtrix/ds-sample/code
      - store_artifacts:
          path: /tmp/data/test4_docker_dsi_mrtrix/ds-sample/derivatives/cmp-*
      - store_artifacts:
          path: /tmp/data/test4_docker_dsi_mrtrix/ds-sample/derivatives/nipype-*

  test5_docker_dsi_dipy:
    machine:
      # Ubuntu 20.04, docker 20.10.11, docker-compose 1.29.2
      image: ubuntu-2004:202201-02
    resource_class: large
    working_directory: /tmp/data
    environment:
      - FS_LICENSE: /tmp/data/${CIRCLE_JOB}/ds-sample/code/license.txt
    steps:
      - run:
          name: Get precise CPU specifications
          command: |
            cat  /proc/cpuinfo
      - checkout:
          path: /home/circleci/src/connectomemapper3
      - run:
          name: Check whether test_docker_dsi_dipy should be skipped
          command: |
            cd /home/circleci/src/connectomemapper3
            if [[ "$( git log --format=oneline -n 1 $CIRCLE_SHA1 | grep -i -E '\[skip[ _]?test_docker_dsi_dipy\]' )" != "" ]]; then
              echo "Skipping test_docker_dsi_dipy job"
              circleci step halt
            fi
      - attach_workspace:
          at: /tmp/cache/docker
      - restore_cache:
          keys:
            - data-ds-sample-v2-4-{{ .Branch }}-{{ .Revision }}
            - data-ds-sample-v2-4--{{ .Revision }}
            - data-ds-sample-v2-4-{{ .Branch }}-
            - data-ds-sample-v2-4-master-
            - data-ds-sample-v2-4-
          paths:
            - /tmp/data/ds-sample
      - run:
          name: "Copy anatomical pipeline output from test 1"
          command: |
            mkdir -p /tmp/data/${CIRCLE_JOB}/ds-sample/derivatives
            cp -r /tmp/data/test1_docker_parcellation/ds-sample/derivatives/nipype-* \
            /tmp/data/${CIRCLE_JOB}/ds-sample/derivatives
            rm -r /tmp/data/ds-sample/derivatives/freesurfer-*
            cp -r /tmp/data/test1_docker_parcellation/ds-sample/derivatives/freesurfer-* \
            /tmp/data/${CIRCLE_JOB}/ds-sample/derivatives
      - load-docker-image-archive:
          archive_file: "/tmp/cache/docker/docker.tar.zst"
      - install-conda-and-cmp3:
          run_name: "Setup miniconda environment and install connectomemapper3 with Python wrappers"
          cmp3_dir: /home/circleci/src/connectomemapper3
      - prepare-ds-sample-dataset:
          circleci_job: ${CIRCLE_JOB}
      - test-docker-anat-dwi-pipelines-wrapper:
          circleci_job: ${CIRCLE_JOB}
          run_name: Run anatomical and diffusion pipelines on ds-sample (test 5 - Dipy SHORE + Deterministic tracking)
          test_name: "test-05"
          config_dir: /home/circleci/src/connectomemapper3/.circleci/tests/configuration_files
          anat_pipeline_config: "ref_anatomical_config_1.ini"
          dwi_pipeline_config: "ref_diffusion_config_3.ini"
          check_output: true
      - persist_to_workspace:
          root: /tmp/data
          paths:
            - ds-sample/test/test-05_*
      - store_artifacts:
          path: /tmp/data/ds-sample/test
      - store_artifacts:
          path: /tmp/data/test5_docker_dsi_dipy/ds-sample/code
      - store_artifacts:
          path: /tmp/data/test5_docker_dsi_dipy/ds-sample/derivatives/cmp-*
      - store_artifacts:
          path: /tmp/data/test5_docker_dsi_dipy/ds-sample/derivatives/nipype-*

  test6_docker_dsi_dipy:
    machine:
      # Ubuntu 20.04, docker 20.10.11, docker-compose 1.29.2
      image: ubuntu-2004:202201-02
    resource_class: large
    working_directory: /tmp/data
    environment:
      - FS_LICENSE: /tmp/data/${CIRCLE_JOB}/ds-sample/code/license.txt
    steps:
      - run:
          name: Get precise CPU specifications
          command: |
            cat  /proc/cpuinfo
      - checkout:
          path: /home/circleci/src/connectomemapper3
      - run:
          name: Check whether test_docker_dsi_dipy should be skipped
          command: |
            cd /home/circleci/src/connectomemapper3
            if [[ "$( git log --format=oneline -n 1 $CIRCLE_SHA1 | grep -i -E '\[skip[ _]?test_docker_dsi_dipy\]' )" != "" ]]; then
              echo "Skipping test_docker_dsi_dipy job"
              circleci step halt
            fi
      - attach_workspace:
          at: /tmp/cache/docker
      - restore_cache:
          keys:
            - data-ds-sample-v2-4-{{ .Branch }}-{{ .Revision }}
            - data-ds-sample-v2-4--{{ .Revision }}
            - data-ds-sample-v2-4-{{ .Branch }}-
            - data-ds-sample-v2-4-master-
            - data-ds-sample-v2-4-
          paths:
            - /tmp/data/ds-sample
      - run:
          name: "Copy anatomical pipeline output from test 1"
          command: |
            mkdir -p /tmp/data/${CIRCLE_JOB}/ds-sample/derivatives
            cp -r /tmp/data/test1_docker_parcellation/ds-sample/derivatives/nipype-* \
            /tmp/data/${CIRCLE_JOB}/ds-sample/derivatives
            rm -r /tmp/data/ds-sample/derivatives/freesurfer-*
            cp -r /tmp/data/test1_docker_parcellation/ds-sample/derivatives/freesurfer-* \
            /tmp/data/${CIRCLE_JOB}/ds-sample/derivatives
      - load-docker-image-archive:
          archive_file: "/tmp/cache/docker/docker.tar.zst"
      - install-conda-and-cmp3:
          run_name: "Setup miniconda environment and install connectomemapper3 with Python wrappers"
          cmp3_dir: /home/circleci/src/connectomemapper3
      - prepare-ds-sample-dataset:
          circleci_job: ${CIRCLE_JOB}
      - test-docker-anat-dwi-pipelines-wrapper:
          circleci_job: ${CIRCLE_JOB}
          run_name: Run anatomical and diffusion pipelines on ds-sample (test 6 - Dipy SHORE + PFT tracking)
          test_name: "test-06"
          config_dir: /home/circleci/src/connectomemapper3/.circleci/tests/configuration_files
          anat_pipeline_config: "ref_anatomical_config_1.ini"
          dwi_pipeline_config: "ref_diffusion_config_4.ini"
          check_output: true
      - persist_to_workspace:
          root: /tmp/data
          paths:
            - ds-sample/test/test-06_*
      - store_artifacts:
          path: /tmp/data/ds-sample/test
      - store_artifacts:
          path: /tmp/data/test6_docker_dsi_dipy/ds-sample/code
      - store_artifacts:
          path: /tmp/data/test6_docker_dsi_dipy/ds-sample/derivatives/cmp-*
      - store_artifacts:
          path: /tmp/data/test6_docker_dsi_dipy/ds-sample/derivatives/nipype-*

  test7_docker_fmri:
    machine:
      # Ubuntu 20.04, docker 20.10.11, docker-compose 1.29.2
      image: ubuntu-2004:202201-02
    resource_class: large
    working_directory: /tmp/data
    environment:
      - FS_LICENSE: /tmp/data/${CIRCLE_JOB}/ds-sample/code/license.txt
    steps:
      - run:
          name: Get precise CPU specifications
          command: |
            cat  /proc/cpuinfo
      - checkout:
          path: /home/circleci/src/connectomemapper3
      - run:
          name: Check whether test_docker_fmri should be skipped
          command: |
            cd /home/circleci/src/connectomemapper3
            if [[ "$( git log --format=oneline -n 1 $CIRCLE_SHA1 | grep -i -E '\[skip[ _]?test_docker_fmri\]' )" != "" ]]; then
              echo "Skipping test_docker_fmri job"
              circleci step halt
            fi
      - attach_workspace:
          at: /tmp/cache/docker
      - restore_cache:
          keys:
            - data-ds-sample-v2-4-{{ .Branch }}-{{ .Revision }}
            - data-ds-sample-v2-4--{{ .Revision }}
            - data-ds-sample-v2-4-{{ .Branch }}-
            - data-ds-sample-v2-4-master-
            - data-ds-sample-v2-4-
          paths:
            - /tmp/data/ds-sample
      - run:
          name: "Copy anatomical pipeline output from test 1"
          command: |
            mkdir -p /tmp/data/${CIRCLE_JOB}/ds-sample/derivatives
            cp -r /tmp/data/test1_docker_parcellation/ds-sample/derivatives/nipype-* \
            /tmp/data/${CIRCLE_JOB}/ds-sample/derivatives
            rm -r /tmp/data/ds-sample/derivatives/freesurfer-*
            cp -r /tmp/data/test1_docker_parcellation/ds-sample/derivatives/freesurfer-* \
            /tmp/data/${CIRCLE_JOB}/ds-sample/derivatives
      - load-docker-image-archive:
          archive_file: "/tmp/cache/docker/docker.tar.zst"
      - install-conda-and-cmp3:
          run_name: "Setup miniconda environment and install connectomemapper3 with Python wrappers"
          cmp3_dir: /home/circleci/src/connectomemapper3
      - prepare-ds-sample-dataset:
          circleci_job: ${CIRCLE_JOB}
      - test-docker-anat-func-pipelines-wrapper:
          circleci_job: ${CIRCLE_JOB}
          run_name: Run anatomical and fmri pipelines on ds-sample (test 7 - Lausanne2018 + BBRegister + fmri)
          test_name: "test-07"
          config_dir: /home/circleci/src/connectomemapper3/.circleci/tests/configuration_files
          anat_pipeline_config: "ref_anatomical_config_1.ini"
          func_pipeline_config: "ref_fMRI_config_1.ini"
          check_output: true
      - persist_to_workspace:
          root: /tmp/data
          paths:
            - ds-sample/test/test-07_*
      - store_artifacts:
          path: /tmp/data/ds-sample/test
      - store_artifacts:
          path: /tmp/data/test7_docker_fmri/ds-sample/code
      - store_artifacts:
          path: /tmp/data/test7_docker_fmri/ds-sample/derivatives/cmp-*
      - store_artifacts:
          path: /tmp/data/test7_docker_fmri/ds-sample/derivatives/nipype-*

  test8_docker_fmri:
    machine:
      # Ubuntu 20.04, docker 20.10.11, docker-compose 1.29.2
      image: ubuntu-2004:202201-02
    resource_class: large
    working_directory: /tmp/data
    environment:
      - FS_LICENSE: /tmp/data/${CIRCLE_JOB}/ds-sample/code/license.txt
    steps:
      - run:
          name: Get precise CPU specifications
          command: |
            cat  /proc/cpuinfo
      - checkout:
          path: /home/circleci/src/connectomemapper3
      - run:
          name: Check whether test_docker_fmri should be skipped
          command: |
            cd /home/circleci/src/connectomemapper3
            if [[ "$( git log --format=oneline -n 1 $CIRCLE_SHA1 | grep -i -E '\[skip[ _]?test_docker_fmri\]' )" != "" ]]; then
              echo "Skipping test_docker_fmri job"
              circleci step halt
            fi
      - attach_workspace:
          at: /tmp/cache/docker
      - restore_cache:
          keys:
            - data-ds-sample-v2-4-{{ .Branch }}-{{ .Revision }}
            - data-ds-sample-v2-4--{{ .Revision }}
            - data-ds-sample-v2-4-{{ .Branch }}-
            - data-ds-sample-v2-4-master-
            - data-ds-sample-v2-4-
          paths:
            - /tmp/data/ds-sample
      - run:
          name: "Copy anatomical pipeline output from test 1"
          command: |
            mkdir -p /tmp/data/${CIRCLE_JOB}/ds-sample/derivatives
            cp -r /tmp/data/test1_docker_parcellation/ds-sample/derivatives/nipype-* \
            /tmp/data/${CIRCLE_JOB}/ds-sample/derivatives
            rm -r /tmp/data/ds-sample/derivatives/freesurfer-*
            cp -r /tmp/data/test1_docker_parcellation/ds-sample/derivatives/freesurfer-* \
            /tmp/data/${CIRCLE_JOB}/ds-sample/derivatives
      - load-docker-image-archive:
          archive_file: "/tmp/cache/docker/docker.tar.zst"
      - install-conda-and-cmp3:
          run_name: "Setup miniconda environment and install connectomemapper3 with Python wrappers"
          cmp3_dir: /home/circleci/src/connectomemapper3
      - prepare-ds-sample-dataset:
          circleci_job: ${CIRCLE_JOB}
      - test-docker-anat-func-pipelines-wrapper:
          circleci_job: ${CIRCLE_JOB}
          run_name: Run anatomical and fmri pipelines on ds-sample (test 8 - Lausanne2018 + FSL FLIRT + All nuisance regression + linear detrending + scrubbing)
          test_name: "test-08"
          config_dir: /home/circleci/src/connectomemapper3/.circleci/tests/configuration_files
          anat_pipeline_config: "ref_anatomical_config_1.ini"
          func_pipeline_config: "ref_fMRI_config_2.ini"
          check_output: true
      - persist_to_workspace:
          root: /tmp/data
          paths:
            - ds-sample/test/test-08_*
      - store_artifacts:
          path: /tmp/data/ds-sample/test
      - store_artifacts:
          path: /tmp/data/test8_docker_fmri/ds-sample/code
      - store_artifacts:
          path: /tmp/data/test8_docker_fmri/ds-sample/derivatives/cmp-*
      - store_artifacts:
          path: /tmp/data/test8_docker_fmri/ds-sample/derivatives/nipype-*

  test9_singularity_parcellation:
    machine:
      # Ubuntu 20.04, docker 20.10.11, docker-compose 1.29.2
      image: ubuntu-2004:202201-02
    resource_class: large
    working_directory: /tmp/data
    environment:
      - FS_LICENSE: /tmp/data/${CIRCLE_JOB}/ds-sample/code/license.txt
    steps:
      - run:
          name: Get precise CPU specifications
          command: |
            cat  /proc/cpuinfo
      - checkout:
          path: /home/circleci/src/connectomemapper3
      - run:
          name: Check whether test_singularity_parcellation should be skipped
          command: |
            cd /home/circleci/src/connectomemapper3
            if [[ "$( git log --format=oneline -n 1 $CIRCLE_SHA1 | grep -i -E '\[skip[ _]?test_singularity_parcellation\]' )" != "" ]]; then
              echo "Skipping test_singularity_parcellation job"
              circleci step halt
            fi
      - attach_workspace:
          at: /tmp/cache/singularity
      - restore_cache:
          keys:
            - data-ds-sample-v2-4-{{ .Branch }}-{{ .Revision }}
            - data-ds-sample-v2-4--{{ .Revision }}
            - data-ds-sample-v2-4-{{ .Branch }}-
            - data-ds-sample-v2-4-master-
            - data-ds-sample-v2-4-
          paths:
            - /tmp/data/ds-sample
      - singularity/install-go:
          go-version: '1.13'
      - singularity/debian-install-3:
          singularity-version: 3.8.4
      - install-conda-and-cmp3:
          run_name: "Setup miniconda environment and install connectomemapper3 with Python wrappers"
          cmp3_dir: /home/circleci/src/connectomemapper3
      - prepare-ds-sample-dataset:
          circleci_job: ${CIRCLE_JOB}
      - test-singularity-anat-pipeline-wrapper:
          circleci_job: ${CIRCLE_JOB}
          run_name: Run anatomical pipeline on ds-sample (test 9 - Lausanne 2018)
          test_name: "test-09"
          config_dir: /home/circleci/src/connectomemapper3/.circleci/tests/configuration_files
          singularity_image: "/tmp/cache/singularity/connectomemapper3.simg"
          anat_pipeline_config: "ref_anatomical_config_1.ini"
      - run:
          name: Clean working directory
          when: always
          command: |
            sudo chown $(id -un):$(id -gn) -R /tmp/data/${CIRCLE_JOB}/ds-sample
            find /tmp/data/ds-sample/derivatives -not -name "*.svg" -not -name "*.html" -not -name "*.rst" \
                -not -name "*.mat" -not -name "*.gpickle" -not -name "*.lta" -not -name "*.json" -not -name "*.txt" \
                -not -name "*.log" -not -name "*.pklz" -type f -delete
      - persist_to_workspace:
          root: /tmp/data
          paths:
            - ds-sample/test/test-09_*
      - store_artifacts:
          path: /tmp/data/ds-sample/test
      - store_artifacts:
          path: /tmp/data/test9_singularity_parcellation/ds-sample/code
      - store_artifacts:
          path: /tmp/data/test9_singularity_parcellation/ds-sample/derivatives/cmp-*
      - store_artifacts:
          path: /tmp/data/test9_singularity_parcellation/ds-sample/derivatives/nipype-*

  test10_docker_eeg:
    machine:
      # Ubuntu 20.04, docker 20.10.11, docker-compose 1.29.2
      image: ubuntu-2004:202201-02
    resource_class: large
    working_directory: /tmp/data
    environment:
      - FS_LICENSE: /tmp/data/${CIRCLE_JOB}/ds003505/code/license.txt
    steps:
      - run:
          name: Get precise CPU specifications
          command: |
            cat  /proc/cpuinfo
      - checkout:
          path: /home/circleci/src/connectomemapper3
      - run:
          name: Check whether test10_docker_eeg should be skipped
          command: |
            cd /home/circleci/src/connectomemapper3
            if [[ "$( git log --format=oneline -n 1 $CIRCLE_SHA1 | grep -i -E '\[skip[ _]?test10_docker_eeg\]' )" != "" ]]; then
              echo "Skipping test10_docker_eeg job"
              circleci step halt
            fi
      - attach_workspace:
          at: /tmp/cache/docker
      - restore_cache:
          keys:
            - data-ds003505-v2-3-{{ .Branch }}-{{ .Revision }}
            - data-ds003505-v2-3--{{ .Revision }}
            - data-ds003505-v2-3-{{ .Branch }}-
            - data-ds003505-v2-3-master-
            - data-ds003505-v2-3-
          paths:
            - /tmp/data/ds003505
      - load-docker-image-archive:
          archive_file: "/tmp/cache/docker/docker.tar.zst"
      - install-conda-and-cmp3:
          run_name: "Setup miniconda environment and install connectomemapper3 with Python wrappers"
          cmp3_dir: /home/circleci/src/connectomemapper3
      - prepare-ds003505-dataset:
          circleci_job: ${CIRCLE_JOB}
      - test-docker-anat-eeg-pipelines-wrapper:
          circleci_job: ${CIRCLE_JOB}
          run_name: Run anatomical and eeg pipelines on ds003505 - VEPCON dataset (test 10 - Lausanne2018 + MNE)
          test_name: "test-10"
          dataset: "ds003505"
          config_dir: /home/circleci/src/connectomemapper3/.circleci/tests/configuration_files
          anat_pipeline_config: "ref_anatomical_config_1.json"
          eeg_pipeline_config: "ref_eeg_config_1.json"
          check_output: true
      - persist_to_workspace:
          root: /tmp/data
          paths:
            - ds003505/test/test-10_*
      - store_artifacts:
          path: /tmp/data/ds003505/test
      - store_artifacts:
          path: /tmp/data/test10_docker_eeg/ds003505/code
      - store_artifacts:
          path: /tmp/data/test10_docker_eeg/ds003505/derivatives/cmp-*
      - store_artifacts:
          path: /tmp/data/test10_docker_eeg/ds003505/derivatives/nipype-*

  codacy_coverage_report:
    docker:
      - image: 'cimg/openjdk:8.0.312'
    working_directory: /tmp/data/ds-sample/test
    steps:
      - attach_workspace:
          at: /tmp/data
      - run:
          name: Run Codacy Coverage Reporter to convert and upload the reports to Codacy
          command: |
            export CODACY_REPORTER_VERSION=latest
            export CODACY_PROJECT_TOKEN="${CODACY_PROJECT_TOKEN}"
            export COMMIT_UUID="${CIRCLE_SHA1}"
            echo "CODACY_REPORTER_VERSION = ${CODACY_REPORTER_VERSION}"
            echo "CODACY_PROJECT_TOKEN = ${CODACY_PROJECT_TOKEN}"
            echo "COMMIT_UUID = ${COMMIT_UUID}"
            # Handles partial reports and send the final report to Codacy
            bash <(curl -Ls https://coverage.codacy.com/get.sh) report \
                --project-token ${CODACY_PROJECT_TOKEN} --commit-uuid ${COMMIT_UUID} \
                -l Python -r /tmp/data/ds-sample/test/test-01_coverage.xml --partial &&\
            bash <(curl -Ls https://coverage.codacy.com/get.sh) report \
                --project-token ${CODACY_PROJECT_TOKEN} --commit-uuid ${COMMIT_UUID} \
                -l Python -r /tmp/data/ds-sample/test/test-02_coverage.xml --partial &&\
            bash <(curl -Ls https://coverage.codacy.com/get.sh) report \
                --project-token ${CODACY_PROJECT_TOKEN} --commit-uuid ${COMMIT_UUID} \
                -l Python -r /tmp/data/ds-sample/test/test-03_coverage.xml --partial &&\
            bash <(curl -Ls https://coverage.codacy.com/get.sh) report \
                --project-token ${CODACY_PROJECT_TOKEN} --commit-uuid ${COMMIT_UUID} \
                -l Python -r /tmp/data/ds-sample/test/test-04_coverage.xml --partial &&\
            bash <(curl -Ls https://coverage.codacy.com/get.sh) report \
                --project-token ${CODACY_PROJECT_TOKEN} --commit-uuid ${COMMIT_UUID} \
                -l Python -r /tmp/data/ds-sample/test/test-05_coverage.xml --partial &&\
            bash <(curl -Ls https://coverage.codacy.com/get.sh) report \
                --project-token ${CODACY_PROJECT_TOKEN} --commit-uuid ${COMMIT_UUID} \
                -l Python -r /tmp/data/ds-sample/test/test-06_coverage.xml --partial &&\
            bash <(curl -Ls https://coverage.codacy.com/get.sh) report \
                --project-token ${CODACY_PROJECT_TOKEN} --commit-uuid ${COMMIT_UUID} \
                -l Python -r /tmp/data/ds-sample/test/test-07_coverage.xml --partial &&\
            bash <(curl -Ls https://coverage.codacy.com/get.sh) report \
                --project-token ${CODACY_PROJECT_TOKEN} --commit-uuid ${COMMIT_UUID} \
                -l Python -r /tmp/data/ds-sample/test/test-08_coverage.xml --partial &&\
            bash <(curl -Ls https://coverage.codacy.com/get.sh) report \
                --project-token ${CODACY_PROJECT_TOKEN} --commit-uuid ${COMMIT_UUID} \
                -l Python -r /tmp/data/ds-sample/test/test-09_coverage.xml --partial &&\
            bash <(curl -Ls https://coverage.codacy.com/get.sh) report \
                --project-token ${CODACY_PROJECT_TOKEN} --commit-uuid ${COMMIT_UUID} \
                -l Python -r /tmp/data/ds003505/test/test-10_coverage.xml --partial &&\
            bash <(curl -Ls https://coverage.codacy.com/get.sh) final \
                --project-token ${CODACY_PROJECT_TOKEN} --commit-uuid ${COMMIT_UUID}

  send_codecov_report:
    docker:
      - image: 'cimg/openjdk:8.0.312'
    working_directory: /home/circleci/src/connectomemapper3
    steps:
      - attach_workspace:
          at: /tmp/data
      - checkout:
          path: /home/circleci/src/connectomemapper3
      - codecov/upload:
          file: /tmp/data/ds-sample/test/test-01_coverage.xml
      - codecov/upload:
          file: /tmp/data/ds-sample/test/test-02_coverage.xml
      - codecov/upload:
          file: /tmp/data/ds-sample/test/test-03_coverage.xml
      - codecov/upload:
          file: /tmp/data/ds-sample/test/test-04_coverage.xml
      - codecov/upload:
          file: /tmp/data/ds-sample/test/test-05_coverage.xml
      - codecov/upload:
          file: /tmp/data/ds-sample/test/test-06_coverage.xml
      - codecov/upload:
          file: /tmp/data/ds-sample/test/test-07_coverage.xml
      - codecov/upload:
          file: /tmp/data/ds-sample/test/test-08_coverage.xml
      - codecov/upload:
          file: /tmp/data/ds-sample/test/test-09_coverage.xml
      - codecov/upload:
          file: /tmp/data/ds003505/test/test-10_coverage.xml

  build_docs:
    machine:
      # Ubuntu 20.04, docker 20.10.11, docker-compose 1.29.2
      image: ubuntu-2004:202201-02
    working_directory: /home/circleci/out/docs
    steps:
      - checkout:
          path: /home/circleci/src/connectomemapper3
      - run:
          name: Check whether build should be skipped
          command: |
            cd /home/circleci/src/connectomemapper3
            if [[ "$( git log --format=oneline -n 1 $CIRCLE_SHA1 | grep -i -E '\[skip[ _]?docs\]' )" != "" ]]; then
              echo "Skipping documentation build job"
              circleci step halt
            fi

      - attach_workspace:
          at: /tmp/cache/docker
      - load-docker-image-archive:
          archive_file: "/tmp/cache/docker/docker.tar.zst"
      - run:
          name: Build Connectome Mapper 3 documentation
          no_output_timeout: 2h
          command: |
            docker run -ti --rm=false -v $PWD:/_build_html \
              --entrypoint=sphinx-build sebastientourbier/connectomemapper3:latest \
              -T -E -b html -d _build/doctrees-readthedocs -W -D \
              language=en /root/src/connectomemapper3/docs/ /_build_html 2>&1 \
              | tee $PWD/builddocs.log
            cat $PWD/builddocs.log
            grep -qv "ERROR" $PWD/builddocs.log
      - store_artifacts:
          path: /home/circleci/out/docs

  deploy_docker_latest:
    machine:
      # Ubuntu 20.04, docker 20.10.11, docker-compose 1.29.2
      image: ubuntu-2004:202201-02
    resource_class: medium
    working_directory: /home/circleci/src/connectomemapper3
    steps:
      - checkout:
          path: /home/circleci/src/connectomemapper3
      - attach_workspace:
          at: /tmp/cache/docker
      - load-docker-image-archive:
          archive_file: "/tmp/cache/docker/docker.tar.zst"
      - run:
          name: Deploy latest master to Docker Hub
          no_output_timeout: 40m
          command: |
            # Get version, update files.
            THISVERSION=$( python /home/circleci/src/connectomemapper3/get_version.py )
            echo "THISVERSION : ${THISVERSION}"
            echo "CIRCLE_BRANCH : ${CIRCLE_BRANCH}"

            if [[ -n "$DOCKER_PASS" ]]; then
              docker login -u $DOCKER_USER -p $DOCKER_PASS
              docker tag sebastientourbier/connectomemapper3 sebastientourbier/connectomemapper-bidsapp:latest
              docker push sebastientourbier/connectomemapper-bidsapp:latest
            fi

  deploy_singularity_latest:
    machine:
      # Ubuntu 20.04, docker 20.10.11, docker-compose 1.29.2
      image: ubuntu-2004:202201-02
    resource_class: medium
    working_directory: /home/circleci/src/connectomemapper3
    steps:
      - checkout:
          path: /home/circleci/src/connectomemapper3
      - run:
          name: "Check whether test should be skipped"
          command: |
            cd /home/circleci/src/connectomemapper3
            if [[ "$( git log --format=oneline -n 1 $CIRCLE_SHA1 | grep -i -E '\[skip[ _]?deploy_singularity]' )" != "" ]]; then
              echo "Skipping singularity deployment"
              circleci step halt
            fi
      - attach_workspace:
          at: /tmp/cache/singularity
      - singularity/install-go:
          go-version: '1.13'
      - singularity/debian-install-3:
          singularity-version: 3.8.4
      - run:
          name: 'Write Sylabs.io TOKEN'
          command: |
            mkdir -p ~/.singularity
            touch ~/.singularity/sylabs-token
            echo "${SINGULARITY_TOKEN}" > ~/.singularity/sylabs-token
            cat ~/.singularity/sylabs-token
            # https://sylabs.io/guides/3.1/user-guide/cli/singularity_remote_login.html
            singularity remote login --tokenfile ~/.singularity/sylabs-token SylabsCloud
      - run:
          name: 'Deploy simg with latest tag to Sylabs.io'
          no_output_timeout: 40m
          command: |
            # singularity delete --arch=amd64 library://connectomicslab/default/connectomemapper-bidsapp:latest
            singularity push -U /tmp/cache/singularity/connectomemapper3.simg library://connectomicslab/default/connectomemapper-bidsapp:latest

  deploy_docker_release:
    machine:
      # Ubuntu 20.04, docker 20.10.11, docker-compose 1.29.2
      image: ubuntu-2004:202201-02
    resource_class: medium
    working_directory: /home/circleci/src/connectomemapper3
    steps:
      - checkout:
          path: /home/circleci/src/connectomemapper3
      - attach_workspace:
          at: /tmp/cache/docker
      - load-docker-image-archive:
          archive_file: "/tmp/cache/docker/docker.tar.zst"
      - run:
          name: Deploy release with version tag to Docker Hub
          no_output_timeout: 40m
          command: |
            # Get version, update files.
            THISVERSION=$( python /home/circleci/src/connectomemapper3/get_version.py )
            echo "THISVERSION : ${THISVERSION}"
            echo "CIRCLE_TAG : ${CIRCLE_TAG}"

            if [[ -n "$DOCKER_PASS" ]]; then
              docker login -u $DOCKER_USER -p $DOCKER_PASS
              if [[ -n "$CIRCLE_TAG" ]]; then
                docker tag sebastientourbier/connectomemapper3 sebastientourbier/connectomemapper-bidsapp:$CIRCLE_TAG
                docker push sebastientourbier/connectomemapper-bidsapp:$CIRCLE_TAG
              fi
            fi

  deploy_singularity_release:
    machine:
      # Ubuntu 20.04, docker 20.10.11, docker-compose 1.29.2
      image: ubuntu-2004:202201-02
    resource_class: medium
    working_directory: /home/circleci/src/connectomemapper3
    steps:
      - checkout:
          path: /home/circleci/src/connectomemapper3
      - run:
          name: "Check whether test should be skipped"
          command: |
            cd /home/circleci/src/connectomemapper3
            if [[ "$( git log --format=oneline -n 1 $CIRCLE_SHA1 | grep -i -E '\[skip[ _]?deploy_singularity]' )" != "" ]]; then
              echo "Skipping singularity deployment"
              circleci step halt
            fi
      - attach_workspace:
          at: /tmp/cache/singularity
      - singularity/install-go:
          go-version: '1.13'
      - singularity/debian-install-3:
          singularity-version: 3.8.4
      - run:
          name: 'Write Sylabs.io TOKEN'
          command: |
            mkdir -p ~/.singularity
            touch ~/.singularity/sylabs-token
            echo "${SINGULARITY_TOKEN}" > ~/.singularity/sylabs-token
            cat ~/.singularity/sylabs-token
            # https://sylabs.io/guides/3.1/user-guide/cli/singularity_remote_login.html
            singularity remote login --tokenfile ~/.singularity/sylabs-token SylabsCloud
      - run:
          name: 'Deploy simg with version tag to Sylabs.io'
          no_output_timeout: 40m
          command: |
            # Get version, update files.
            # FIXME: cannot use python (might need to install extra package with apt-get)
            # THISVERSION="$( python /home/circleci/src/connectomemapper3/get_version.py )"
            # echo "THISVERSION : ${THISVERSION}"
            echo "CIRCLE_TAG : ${CIRCLE_TAG}"
            singularity push -U /tmp/cache/singularity/connectomemapper3.simg library://connectomicslab/default/connectomemapper-bidsapp:${CIRCLE_TAG}

workflows:
  version: 2.1
  build_test_cov_deploy:
    jobs:
      # JOB taking maximum 1hour (CircleCI hard limit)

      - test-python-install:
          version: "3.7"
          filters:
            branches:
              ignore:
                - paper-joss
                - pull/142
            tags:
              only: /.*/

      - build_docker:
          requires:
            - test-python-install
          filters:
            branches:
              ignore:
                - paper-joss
                - pull/142
            tags:
              only: /.*/

      - build_singularity:
          requires:
            - build_docker
          filters:
            branches:
              ignore:
                - paper-joss
                - pull/142
            tags:
              only: /.*/

      - get_data:
          filters:
            branches:
              ignore:
                - /docs?\/.*/
                - paper-joss
                - pull/142
            tags:
              only: /.*/

      - test1_docker_parcellation:
          requires:
            - get_data
            - build_docker
          filters:
            branches:
              ignore:
                - /docs?\/.*/
                - paper-joss
                - pull/142
            tags:
              only: /.*/

      - test2_docker_parcellation:
          requires:
            - get_data
            - build_docker
          filters:
            branches:
              ignore:
                - /docs?\/.*/
                - paper-joss
                - pull/142
            tags:
              only: /.*/

      - test3_docker_dsi_mrtrix:
          requires:
            - test1_docker_parcellation
          filters:
            branches:
              ignore:
                - /docs?\/.*/
                - paper-joss
                - pull/142
            tags:
              only: /.*/

      - test4_docker_dsi_mrtrix:
          requires:
            - test1_docker_parcellation
          filters:
            branches:
              ignore:
                - /docs?\/.*/
                - paper-joss
                - pull/142
            tags:
              only: /.*/

      - test5_docker_dsi_dipy:
          requires:
            - test1_docker_parcellation
          filters:
            branches:
              ignore:
                - /docs?\/.*/
                - paper-joss
                - pull/142
            tags:
              only: /.*/

      - test6_docker_dsi_dipy:
          requires:
            - test1_docker_parcellation
          filters:
            branches:
              ignore:
                - /docs?\/.*/
                - paper-joss
                - pull/142
            tags:
              only: /.*/

      - test7_docker_fmri:
          requires:
            - test1_docker_parcellation
          filters:
            branches:
              ignore:
                - /docs?\/.*/
                - paper-joss
                - pull/142
            tags:
              only: /.*/

      - test8_docker_fmri:
          requires:
            - test1_docker_parcellation
          filters:
            branches:
              ignore:
                - /docs?\/.*/
                - paper-joss
                - pull/142
            tags:
              only: /.*/

      - test9_singularity_parcellation:
          requires:
            - get_data
            - build_singularity
          filters:
            branches:
              ignore:
                - /docs?\/.*/
                - paper-joss
                - pull/142
            tags:
              only: /.*/

      - test10_docker_eeg:
          requires:
            - get_data
            - build_docker
          filters:
            branches:
              ignore:
                - /docs?\/.*/
                - paper-joss
                - pull/142
            tags:
              only: /.*/

      - codacy_coverage_report:
          requires:
            - test1_docker_parcellation
            - test2_docker_parcellation
            - test3_docker_dsi_mrtrix
            - test4_docker_dsi_mrtrix
            - test5_docker_dsi_dipy
            - test6_docker_dsi_dipy
            - test7_docker_fmri
            - test8_docker_fmri
            - test9_singularity_parcellation
            - test10_docker_eeg
          filters:
              branches:
                ignore:
                  - /docs?\/.*/
                  - paper-joss
                  - pull/142
              tags:
                only: /.*/

      - send_codecov_report:
          requires:
            - test1_docker_parcellation
            - test2_docker_parcellation
            - test3_docker_dsi_mrtrix
            - test4_docker_dsi_mrtrix
            - test5_docker_dsi_dipy
            - test6_docker_dsi_dipy
            - test7_docker_fmri
            - test8_docker_fmri
            - test9_singularity_parcellation
            - test10_docker_eeg
          filters:
            branches:
              ignore:
                - /docs?\/.*/
                - paper-joss
                - pull/142
            tags:
              only: /.*/

      - deploy-pypi-release:
          requires:
            - codacy_coverage_report
            - send_codecov_report
          filters:
            # ignore any commit on any branch by default
            branches:
              ignore: /.*/
              # only: master
            # only act on version tags
            tags:
              only: /^v.*/

      - deploy_docker_release:
          requires:
            - codacy_coverage_report
            - send_codecov_report
          filters:
            # ignore any commit on any branch by default
            branches:
              ignore: /.*/
              # only: master
            # only act on version tags
            tags:
              only: /^v.*/

      - deploy_docker_latest:
          requires:
            - codacy_coverage_report
            - send_codecov_report
          filters:
            # ignore any commit on any branch by default
            branches:
              only: master

      - deploy_singularity_release:
          requires:
            - codacy_coverage_report
            - send_codecov_report
          filters:
            # ignore any commit on any branch by default
            branches:
              ignore: /.*/
              # only: master
            # only act on version tags
            tags:
              only: /^v.*/
